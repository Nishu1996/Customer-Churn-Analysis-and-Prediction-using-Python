# -*- coding: utf-8 -*-
"""CS577_Project_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iAHqQIw89tPmTu_3YzCtjAfjSOlsffsO

***Customer Churn Analysis & Prediction***

**Target Variable**: Churn
"""

#importing all required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from pandas.core.api import DataFrame
#reading the data
df=pd.read_csv("E_Commerce_Dataset.csv")
#df = pd.read_excel('/content/E Commerce Dataset.xlsx', sheet_name='E Comm')

print("Dataset shape:", df.shape)
print("Columns:", df.columns.values)
df.head()

# data types of each column
print("Data types:\n")
df.dtypes

# summary statistics of dataset
df.describe()

# check if dataset contains any NaN values
print("NaN values:")

df.isnull().sum()

# handling Nan values
# removing rows with Nan values
new_df = df.dropna(subset=['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder'])

# ensuring Nan values do not exist

new_df.isnull().sum()

# understanding target variable (Churn)
print("Value counts for Churn variable:",new_df['Churn'].value_counts())

# visualizing value counts for Churn variable
new_df['Churn'].value_counts().plot(kind='bar', color=['skyblue', 'orange'])
plt.title('Churn Variable Value Counts')
plt.xlabel('Churn')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# dividing data based on data type

categorical = ['PreferredLoginDevice', 'PreferredPaymentMode', 'PreferedOrderCat', 'MaritalStatus']
numerical = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'NumberOfDeviceRegistered', 'NumberOfAddress',
            'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder', 'CashbackAmount', 'CityTier', 'SatisfactionScore','Complain']

interval = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'NumberOfDeviceRegistered', 'NumberOfAddress',
            'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder', 'CashbackAmount']

ordinal = ['PreferredLoginDevice', 'PreferredPaymentMode', 'Gender', 'PreferedOrderCat', 'MaritalStatus']
nominal = ['Churn', 'CityTier', 'SatisfactionScore','Complain']

# Statistics Summary for Categorical Data
new_df[categorical].describe()

# value counts for categorical values
for col in categorical:
  print(f'''Value count Colums {col}:''')
  print(df[col].value_counts())
  print()

# Statistics Summary for Numerical Data
new_df[numerical].describe()

# Statistics Summary for Interval Data
new_df[interval].describe()

# Statistics Summary for Nominal Data
new_df[ordinal].describe()

# correlation plot to understand which variables are significant
# sns.heatmap(new_df.corr(), annot=True)
new_df.corr()

"""From the above table and below heapmap, we can observe that 'Churn' variable:

1.   Has positive correlation with CityTier, WarehouseToHome, NumberOfDeviceRegistered, SatisfactionScore and Complain.
2.   Has a very weak correlation (almost near 0) with HourSpendOnApp,NumberOfAddress, OrderAmoundHikeFromlastYear, CouponUsed and OrderCount. This clearly indicates that the above features may not be significant.
"""

# Correlation Analysis
plt.figure(figsize=(16, 8))

# Create mask for diagonal correlation plot
mask = np.triu(np.ones_like(new_df.corr(), dtype=np.bool))

# Plot the heatmap
heatmap = sns.heatmap(new_df.corr(), mask=mask, annot=True, cmap='Blues' )
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16);

"""**Modeling**"""

# split the dataset into train & test
x = new_df.drop(columns=['CustomerID','Churn'])
y = new_df['Churn']

from sklearn.model_selection import train_test_split
# test data will contain 20% of the dataset and train will contain 70% of the dataset
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# label encoding
!pip install feature_engine
from feature_engine.encoding import OneHotEncoder
from feature_engine.encoding import OrdinalEncoder
def label_encoding(x_train,
                   x_test):
  ordinal_encoder = OrdinalEncoder(encoding_method='arbitrary',
variables='Gender')
  ordinal_encoder.fit(x_train)
  x_train = ordinal_encoder.transform(x_train)
  x_test = ordinal_encoder.transform(x_test)
  return x_train, x_test

x_train, x_test = label_encoding(x_train,x_test)

# one hot encoding
def label_ohe(x_train,
              x_test):
  cat_features = ['PreferredLoginDevice', 'PreferredPaymentMode', 'PreferedOrderCat', 'MaritalStatus']
  ohe_encoder = OneHotEncoder(variables=cat_features)
  ohe_encoder.fit(x_train)
  x_train = ohe_encoder.transform(x_train)
  x_test = ohe_encoder.transform(x_test)

  return x_train, x_test

x_train , x_test=label_ohe(x_train, x_test)
x_train

import sklearn.linear_model as lm
model = lm.LogisticRegression(penalty = 'none', fit_intercept = False, solver = 'lbfgs')

# fitting the model
model.fit(x_train, y_train)

# assessing the model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
y_pred = model.predict(x_test)
confusion = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
accuracy_lr = accuracy_score(y_test, y_pred)
confusion_matrix_lr = confusion_matrix(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print("Accuracy:", accuracy_lr )
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)

# UNPRUNED TREE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score

model_decision_tree = DecisionTreeClassifier(random_state=42)

model_decision_tree.fit(x_train, y_train)

# Make predictions
y_pred_decision_tree = model_decision_tree.predict(x_test)

# Model evaluation
accuracy_decision_tree = accuracy_score(y_test, y_pred_decision_tree)
confusion_matrix_decision_tree = confusion_matrix(y_test, y_pred_decision_tree)
precision = precision_score(y_test, y_pred_decision_tree)
recall = recall_score(y_test, y_pred_decision_tree)
f1 = f1_score(y_test, y_pred_decision_tree)

print("Unpruned Decision Tree Accuracy:", accuracy_decision_tree)
print("Unpruned Decision Tree Confusion Matrix:\n", confusion_matrix_decision_tree)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)

# Predicting the train data
y_train_decision_tree = model_decision_tree.predict(x_train)

# Model evaluation
accuracy_decision_tree_train = accuracy_score(y_train, y_train_decision_tree)

print("Training accuracy: ",accuracy_decision_tree_train)

## PRUNED DECISION TREE

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score

# Preventing tree growth - don't split nodes with < 1% of the samples , max_depth = 7
min_samples_split = int(0.01 * len(x_train))

model_decision_tree = DecisionTreeClassifier(min_samples_split=min_samples_split,max_depth=7,random_state=42)

model_decision_tree.fit(x_train, y_train)

y_pred_decision_tree = model_decision_tree.predict(x_test)

# Model evaluation
accuracy_decision_tree = accuracy_score(y_test, y_pred_decision_tree)
confusion_matrix_decision_tree = confusion_matrix(y_test, y_pred_decision_tree)
precision = precision_score(y_test, y_pred_decision_tree)
recall = recall_score(y_test, y_pred_decision_tree)
f1 = f1_score(y_test, y_pred_decision_tree)

print("Pruned Decision Tree Accuracy:", accuracy_decision_tree)
print("Pruned Decision Tree Confusion Matrix:\n", confusion_matrix_decision_tree)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)

y_train_decision_tree = model_decision_tree.predict(x_train)

# Model evaluation
accuracy_decision_tree_train = accuracy_score(y_train, y_train_decision_tree)

print("Training accuracy: ",accuracy_decision_tree_train)

## RANDOM FOREST
from sklearn.ensemble import RandomForestClassifier

random_forest_model = RandomForestClassifier(criterion='entropy',
    n_estimators=10,
    random_state=1,
    n_jobs=2)

random_forest_model.fit(x_train, y_train)

# predictions on the test set
y_pred_random_forest = random_forest_model.predict(x_test)

# Model evaluation
accuracy_rf = accuracy_score(y_test, y_pred_random_forest)
precision_rf = precision_score(y_test, y_pred_random_forest)
recall_rf = recall_score(y_test, y_pred_random_forest)
f1_rf = f1_score(y_test, y_pred_random_forest)
confusion_matrix_rf = confusion_matrix(y_test, y_pred_random_forest)

print("Random Forest Accuracy:", accuracy_rf)
print("Random Forest Precision:", precision_rf)
print("Random Forest Recall:", recall_rf)
print("Random Forest F1-Score:", f1_rf)
print("Random Forest Confusion Matrix:\n", confusion_matrix_rf)

"""Observations:

Accuracy: The Random Forest model has a higher accuracy compared to the pruned Decision Tree. This suggests that the Random Forest is performing better on the given test set.

Precision: The precision of the Random Forest is higher, indicating that it has a lower rate of false positives compared to the pruned Decision Tree.

Recall: The pruned Decision Tree has a higher recall, suggesting that it is better at capturing true positive instances compared to the Random Forest.

F1-Score: The F1-Score is a balance between precision and recall. The Random Forest has a higher F1-Score, indicating a better overall balance between precision and recall compared to the pruned Decision Tree.

Training Accuracy: The pruned Decision Tree has a lower training accuracy than the Random Forest, which is expected as pruning usually results in a less complex model.

Deciding which model is "better" depends on the specific goals. If we prioritize precision (minimizing false positives), the Random Forest might be preferable. If capturing all positive instances (maximizing recall) is crucial, the pruned Decision Tree might be more suitable.


"""

# Decision tree feature importance

# Assuming your pruned Decision Tree model is stored in 'model_decision_tree'
feature_importance_decision_tree = model_decision_tree.feature_importances_

# Create a DataFrame to display feature names and their importance scores
df_feature_importance_decision_tree = pd.DataFrame(
    {'Feature': x_train.columns, 'Importance': feature_importance_decision_tree}
)

# Sort the DataFrame by importance in descending order
df_feature_importance_decision_tree = df_feature_importance_decision_tree.sort_values(
    by='Importance', ascending=False
)

# Display the feature importance
print(df_feature_importance_decision_tree)

# Random forest feature importance

# Assuming your Random Forest model is stored in 'random_forest_model'
feature_importance_random_forest = random_forest_model.feature_importances_

# Create a DataFrame to display feature names and their importance scores
df_feature_importance_random_forest = pd.DataFrame(
    {'Feature': x_train.columns, 'Importance': feature_importance_random_forest}
)

# Sort the DataFrame by importance in descending order
df_feature_importance_random_forest = df_feature_importance_random_forest.sort_values(
    by='Importance', ascending=False
)

# Display the feature importance
print(df_feature_importance_random_forest)

pip install --upgrade scikit-learn

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, RocCurveDisplay

# Model Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_random_forest)
roc_auc_rf = roc_auc_score(y_test, y_pred_random_forest)
precision_rf = precision_score(y_test, y_pred_random_forest)
recall_rf = recall_score(y_test, y_pred_random_forest)
f1_rf = f1_score(y_test, y_pred_random_forest)
confusion_matrix_rf = confusion_matrix(y_test, y_pred_random_forest)

# Print Results
print("Random Forest Accuracy:", accuracy_rf)
print("Random Forest ROC Area under Curve:", roc_auc_rf)
print("Random Forest Precision:", precision_rf)
print("Random Forest Recall:", recall_rf)
print("Random Forest F1-Score:", f1_rf)

# Print Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred_random_forest, digits=5))

# Plot Confusion Matrix
plot_confusion_matrix(confusion_matrix_rf)

# Plot ROC Curve
RocCurveDisplay.from_estimator(random_forest_model, x_test, y_test)